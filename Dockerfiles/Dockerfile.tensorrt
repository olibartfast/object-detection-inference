
# Stage 1: Cuda dependencies
FROM nvcr.io/nvidia/cuda:12.3.2-devel-ubuntu22.04 as cuda_dependencies

# note: another alternative would be to use directly a tensorrt image from nvidia ngc

# Set environment variable to avoid interactive prompts
ENV DEBIAN_FRONTEND=noninteractive

# Install common dependencies
RUN apt-get update && apt-get install -y \
    cmake \
    build-essential \
    libopencv-dev \
    libspdlog-dev \
    wget \
    && rm -rf /var/lib/apt/lists/*

# Stage 2: Install backend-specific C++ dependencies
FROM cuda_dependencies AS backend_dependencies

ENV HOME=/root
ARG BACKEND=TENSORRT

ARG TRT_MAJOR=8
ARG TRT_MINOR=.6
ARG TRT_PATCH=.1
ARG TRT_BUILD=.6
ARG TRT_VERSION=$TRT_MAJOR$TRT_MINOR$TRT_PATCH$TRT_BUILD
ARG TENSORRT_DIR=$HOME/TensorRT-$TRT_VERSION 
ARG CUDA_VERSION=12.0

RUN wget https://developer.nvidia.com/downloads/compute/machine-learning/tensorrt/secure/$TRT_MAJOR$TRT_MINOR$TRT_PATCH/tars/TensorRT-$TRT_VERSION.Linux.x86_64-gnu.cuda-$CUDA_VERSION.tar.gz 

RUN tar -xzvf TensorRT-$TRT_VERSION.Linux.x86_64-gnu.cuda-$CUDA_VERSION.tar.gz -C $HOME && \
    rm TensorRT-$TRT_VERSION.Linux.x86_64-gnu.cuda-$CUDA_VERSION.tar.gz


# Stage 3: Copy application code and build
FROM backend_dependencies AS builder

WORKDIR /app

COPY . .

# Build the project using CMake
RUN cmake -Bbuild -H. -DDEFAULT_BACKEND=$BACKEND -DTRT_VERSION=$TRT_VERSION  -DTENSORRT_DIR=$TENSORRT_DIR && \
    cmake --build build --config Release

# Stage 4: Final image
FROM backend_dependencies AS final

COPY --from=builder /app/build/object-detection-inference /app/object-detection-inference

WORKDIR /app

# Set the entry point to the compiled executable
# --entrypoint param to override it from docker run
ENTRYPOINT  ["./object-detection-inference"]
